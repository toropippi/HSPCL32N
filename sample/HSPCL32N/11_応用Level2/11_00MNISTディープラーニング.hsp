//ゼロから作るディープラーニング本のコードに加えBatch normalizationを実装したもの
//SGDのままだが今後Adamに変更予定
//教師=60000枚、テスト=10000枚で現状、正解率97%達成可能できている

#include "hspcl32n.as"
#include "NNmodule.hsp"
	randomize
	clinit

	alpha=0.19
	trainN=60000
	testN=10000
	allN=trainN+testN
	batchN=256//=1batch
	mlpLayerNum=4
	dim mlpLayerNode,mlpLayerNum
	mlpLayerNode=28*28,192,192,10

	gosub*教師テストデータ作成
	gosub*Make_BNbuffer
	gosub*WV作成//重み

	//メイン計算
	//まいループでランダムにチョイスして256ミニバッチ学習
	h_ce=float(0.0)
		repeat 1441
		gosub*Make_rndchiceXT
		//学習
		trainingflg=1
		FNN mlpLayerNode,matX,matWV,BN,trainingflg
		ce=CE_FFN(matT)
		dCE_FNN mlpLayerNode,matX,matWV,BN,matT,alpha,trainingflg
		//テスト
		trainingflg=0
		FNN mlpLayerNode,testX,matWV,BN,trainingflg
		ce_test=CE_FFN(testT)

		if cnt\16==0:gosub*結果表示
		clGarbageCollectionNow
		await 1
		loop

	stop

*結果表示
	if cnt==0{
		outstr="試行回数(ミニバッチ)	Loss_train	Loss_test	正解率\n"
		pos 0,0
		mesbox outstr,636,476,5
		mesboxid=stat
	}

	outstr+=""+cnt+"		"+strf("%1.4f",clReadIndex_fp(ce,0))+"	"+strf("%1.4f",clReadIndex_fp(ce_test,0))+"	"+strf("%1.4f",GetAnsRate(testT))+"\n"
	objprm mesboxid,outstr
	return

*Make_rndchiceXT
	//まず全体から256個の重複なしランダムを作成したいが重複なしは大変なので重複ありで
	dim rn,256
		repeat 256
		rn.cnt=(rnd(8192)*8192+rnd(8192))\trainN
		loop
	//X
		repeat 256
		clCopyBuffer matX,trainX,28*28*4,28*28*4*cnt,28*28*4*rn.cnt
		loop
	//T
		repeat 256
		clCopyBuffer matT,trainT,10*4,10*4*cnt,10*4*rn.cnt
		loop
	return

*Make_BNbuffer
	//Batch norm計算クラス＝モジュール変数
		repeat mlpLayerNum-2
		newmod BN,BatchNormalization,mlpLayerNode.(cnt+1),cnt+1
		loop
	return

*WV作成
	// Heの初期値
	dim matWV,mlpLayerNum-1
		repeat mlpLayerNum-1
		sz=mlpLayerNode.cnt*mlpLayerNode.(cnt+1)
		fdim he,sz
		rsqrt=1.0/sqrt(0.5*mlpLayerNode.cnt)
			repeat sz
			he.cnt=float(randn(rsqrt))
			loop
		matWV.cnt=clCreateBufferFrom(he)
		clBLAS_Set2DShape matWV.cnt,mlpLayerNode.cnt,mlpLayerNode.(cnt+1)
		clIncRefcntCLBufferId matWV.cnt
		loop
	return


*教師テストデータ作成
	//全画像データロード。X
	buffer 1:picload "mnist.png"
	allX=clCreateBuffer(mlpLayerNode.0*allN*4)
	memG=clCreateBuffer(mlpLayerNode.0*700*4)
	
	buffer 2,28,28*700
		repeat 100
		gsel 2
		pos 0,0
		gcopy 1,cnt*28,0,28,28*700
		gselToBufferFloat2 2,memG
		clCopyBuffer allX,memG,mlpLayerNode.0*700*4,mlpLayerNode.0*700*4*cnt,0
		loop

	buffer 1,4,4
	buffer 2,4,4
	
	gsel 0
	
	//全答えロード。T
	dim allAns,allN/4
	bload "ans.txt",allAns
	allT=clCreateBuffer(10*allN*4)
	clFillBuffer allT,float(0)
	clDoXc "F0[(A[i]-48)+i*10]=1.0;",clCreateBufferFrom(allAns),allT

	//教師/////////
	//60000
	//画像
	trainX=clCreateBuffer(trainN*mlpLayerNode.0*4)
	clCopyBuffer trainX,allX,mlpLayerNode.0*trainN*4,0,0
	//答え
	trainT=clCreateBuffer(trainN*mlpLayerNode.(mlpLayerNum-1)*4)
	clCopyBuffer trainT,allT,mlpLayerNode.(mlpLayerNum-1)*trainN*4,0,0
		
	//1batchあたりの計算につかう256サイズ
	//画像
	matX=clCreateBuffer(batchN*mlpLayerNode.0*4)
	clBLAS_Set2DShape matX,batchN,mlpLayerNode.0

	//答え
	matT=clCreateBuffer(batchN*mlpLayerNode.(mlpLayerNum-1)*4)
	clBLAS_Set2DShape matT,batchN,mlpLayerNode.(mlpLayerNum-1)


	//テスト/////////
	//10000
	//画像
	testX=clCreateBuffer(testN*mlpLayerNode.0*4)
	clBLAS_Set2DShape testX,testN,mlpLayerNode.0
	clCopyBuffer testX,allX,mlpLayerNode.0*testN*4,0,mlpLayerNode.0*trainN*4
	//答え
	testT=clCreateBuffer(testN*mlpLayerNode.(mlpLayerNum-1)*4)
	clBLAS_Set2DShape testT,testN,mlpLayerNode.(mlpLayerNum-1)
	clCopyBuffer testT,allT,mlpLayerNode.(mlpLayerNum-1)*testN*4,0,mlpLayerNode.(mlpLayerNum-1)*trainN*4
	
	//
	clIncRefcntCLBufferId matX,matT,trainX,trainT,testX,testT
	return


#module

//正規分布するランダムを返す、ボックスミュラー法
#defcfunc randn double scale
    u1 = 1.0*(rnd(32768)+1)/32768.0
    u2 = 1.0*(rnd(32768)+1)/32768.0
    logU1 = -2.0 * logf(u1)
    sq = sqrt(logU1)
    theta = 3.1415926535898 * 2.0 * u2
    z0 = sq * cos(theta) * scale
    z1 = sq * sin(theta) * scale
return z0

#global